{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "ROOTDIR = os.path.abspath(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "DATADIR = os.path.join(ROOTDIR, 'data')\n",
    "MODELDIR = os.path.join(ROOTDIR, 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "import json\n",
    "import jieba\n",
    "from tqdm import tqdm \n",
    "from itertools import chain\n",
    "\n",
    "def split_sentence(article):\n",
    "    p_b = 0\n",
    "    p_a = 0\n",
    "    re = []\n",
    "    for i in range(len(article)):\n",
    "        if article[i] in ['。','？','！','…','；','，','.','?','!',';',':',',']:\n",
    "            p_a = i\n",
    "            if article[p_b:p_a] != '':\n",
    "                re.append(article[p_b:p_a])\n",
    "            p_b = i + 1\n",
    "    if len(re) == 0:\n",
    "        return [article]\n",
    "    else:\n",
    "        return re\n",
    "    \n",
    "def clean_text(tokenized_list, sw, punct):\n",
    "    new_list = []\n",
    "    print('clean text ...')\n",
    "    for doc in tqdm(tokenized_list):\n",
    "        cleaned_doc = [token.lower() for token in doc if token.lower() not in chain(punct, sw)]\n",
    "        if len(cleaned_doc) > 3:\n",
    "            new_list.append(cleaned_doc)\n",
    "    return new_list\n",
    "\n",
    "def make_cleaned(tokenized):\n",
    "    dictionary = corpora.Dictionary(tokenized)\n",
    "    print ('before:\\t',len(dictionary))\n",
    "    only_once_key=[]\n",
    "    for key in dictionary.iterkeys():\n",
    "        if dictionary.dfs[key]==1:\n",
    "            only_once_key.append(key)\n",
    "    dictionary.filter_tokens(bad_ids=only_once_key)\n",
    "    print ('after:\\t',len(dictionary))\n",
    "    \n",
    "    frequent_tokenized = []\n",
    "    print('remove low frequnce ...')\n",
    "    for sentence in tqdm(tokenized):\n",
    "        ind_list = dictionary.doc2idx(sentence)\n",
    "        words = []\n",
    "        for i,ind in enumerate(ind_list):\n",
    "            if ind != -1:\n",
    "                words.append(sentence[i])\n",
    "        frequent_tokenized.append(words)\n",
    "    punct =  '《》，。？/：；‘’“”{}【】、|—·！~ \\n'+'!\"#%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    sw = []\n",
    "    with open (DATADIR + \"/cn_stopwords.txt\", 'r', encoding='utf8') as f:\n",
    "        sw = f.read().split('\\n')\n",
    "    \n",
    "    return clean_text(frequent_tokenized, sw, punct)\n",
    "\n",
    "def get_tokenized_txt(path,enc):\n",
    "    tokenized = []\n",
    "    for file in tqdm(os.listdir(path)):\n",
    "        with open(path+ '\\\\' + file, 'r',encoding = enc) as f:\n",
    "            try:\n",
    "                lines = f.read().split('\\n')\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "            else:\n",
    "                for line in lines:\n",
    "                    if line != '\\n':\n",
    "                        sentences = split_sentence(line)\n",
    "                        for s in sentences:\n",
    "                            tokenized.append(list(jieba.cut(s, HMM=False)))\n",
    "    return tokenized\n",
    "\n",
    "def get_tokenized_json(path):\n",
    "    tokenized = []\n",
    "    print('get tokenized ...')\n",
    "    for file in tqdm(os.listdir(path)):\n",
    "        with open(path+ '\\\\' + file, 'r', encoding='utf8') as f:\n",
    "            for line in f.readlines():\n",
    "                sentences = []\n",
    "                paragraphs = json.loads(line)['text'].split('\\n\\n')[1:]\n",
    "                for paragraph in paragraphs:\n",
    "                    if paragraph == ' ' or paragraph == '' or paragraph == '\\n':\n",
    "                        break\n",
    "                    tokenized.append(list(jieba.cut(paragraph, HMM=False)))\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 123/123 [00:10<00:00, 11.99it/s]\n",
      "  0%|                                                                                       | 0/419468 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:\t 48355\n",
      "after:\t 33127\n",
      "remove low frequnce ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 419468/419468 [00:01<00:00, 233443.09it/s]\n",
      "  0%|▎                                                                        | 1483/419468 [00:00<00:28, 14777.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean text ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 419468/419468 [00:30<00:00, 13929.06it/s]\n"
     ]
    }
   ],
   "source": [
    "d2_path = DATADIR  + '/轻小说'\n",
    "d2_cleaned = make_cleaned(get_tokenized_txt(d2_path,'utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get tokenized ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:34<00:00,  1.44it/s]\n",
      "  0%|                                                                                       | 0/105587 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:\t 179191\n",
      "after:\t 113665\n",
      "remove low frequnce ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 105587/105587 [00:03<00:00, 30532.43it/s]\n",
      "  0%|                                                                           | 120/105587 [00:00<01:29, 1179.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean text ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 105587/105587 [01:40<00:00, 1048.39it/s]\n"
     ]
    }
   ],
   "source": [
    "wiki_path = DATADIR  + '/维基'\n",
    "wiki_cleaned = make_cleaned(get_tokenized_json(wiki_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:20<00:00,  2.04s/it]\n",
      "  4%|██▉                                                                    | 31580/757965 [00:00<00:02, 313441.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:\t 49813\n",
      "after:\t 38964\n",
      "remove low frequnce ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 757965/757965 [00:03<00:00, 224320.52it/s]\n",
      "  0%|▏                                                                        | 2131/757965 [00:00<00:35, 21149.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean text ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 757965/757965 [00:34<00:00, 21661.74it/s]\n"
     ]
    }
   ],
   "source": [
    "weibo_path = DATADIR  + '/口语'\n",
    "weibo_cleaned = make_cleaned(get_tokenized_txt(weibo_path,'utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:24<00:00,  1.26it/s]\n",
      "  0%|                                                                                      | 0/1050038 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:\t 94478\n",
      "after:\t 70427\n",
      "remove low frequnce ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1050038/1050038 [00:04<00:00, 224405.72it/s]\n",
      "  0%|                                                                        | 1560/1050038 [00:00<01:07, 15482.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean text ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 1050038/1050038 [01:08<00:00, 15308.08it/s]\n"
     ]
    }
   ],
   "source": [
    "tra_path = DATADIR  + '/文学'\n",
    "tra_cleaned = make_cleaned(get_tokenized_txt(tra_path,'utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "all_text_data = d2_cleaned + weibo_cleaned + wiki_cleaned + tra_cleaned\n",
    "\n",
    "w2v_model = models.Word2Vec(all_text_data,\n",
    "                        size = 300,\n",
    "                        window = 5,\n",
    "                        min_count = 1,\n",
    "                        sg = 0,\n",
    "                        alpha = 0.025,\n",
    "                        iter=10,\n",
    "                        batch_words = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save(MODELDIR + \"/word2vec_style.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(d2_cleaned + weibo_cleaned + wiki_cleaned + tra_cleaned)\n",
    "dictionary.save(MODELDIR + '/dictionary_style.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载模型\n",
    "from gensim import models\n",
    "\n",
    "w2v_model = models.Word2Vec.load(MODELDIR + \"/word2vec_style.model\")\n",
    "dictionary = corpora.Dictionary.load(MODELDIR + '/dictionary_style.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_d2 = np.array([w2v_model.wv[q].sum(axis=0)/len(q) for q in d2_cleaned])\n",
    "vec_weibo = np.array([w2v_model.wv[q].sum(axis=0)/len(q) for q in weibo_cleaned])\n",
    "vec_wiki = np.array([w2v_model.wv[q].sum(axis=0)/len(q) for q in wiki_cleaned])\n",
    "vec_tra = np.array([w2v_model.wv[q].sum(axis=0)/len(q) for q in tra_cleaned])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文学性分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(440016, 300)\n",
      "(103659, 300)\n",
      "(189782, 300)\n",
      "(183608, 300)\n"
     ]
    }
   ],
   "source": [
    "print(vec_tra.shape)\n",
    "print(vec_wiki.shape)\n",
    "print(vec_d2.shape)\n",
    "print(vec_weibo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before undersample:\t [(0.0, 477049), (1.0, 440016)]\n",
      "after undersample:\t [(0.0, 440016), (1.0, 440016)]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "\n",
    "X = np.concatenate((vec_d2,vec_weibo,vec_wiki,vec_tra),axis = 0)\n",
    "y =  np.concatenate((np.zeros(vec_d2.shape[0] + vec_weibo.shape[0] + vec_wiki.shape[0]),np.ones(vec_tra.shape[0])),axis = 0)\n",
    "\n",
    "print('before undersample:\\t',sorted(Counter(y).items()))\n",
    "\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "\n",
    "print('after undersample:\\t',sorted(Counter(y_resampled).items()))\n",
    "\n",
    "y_resampled = y_resampled.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 42\n",
    "test_size = 0.33\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "C:\\Users\\frank\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8540103508475919"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr = lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# check the accuracy on the training set\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_score(article,classifier,dictionary,w2v_model):\n",
    "    sentences = split_sentence(article)\n",
    "    \n",
    "    tokenized = []\n",
    "    for sentence in sentences:\n",
    "        tokenized.append(list(jieba.cut(sentence, HMM=False)))\n",
    "    cleaned = []\n",
    "    for sentence in tokenized:\n",
    "        ind_list = dictionary.doc2idx(sentence)\n",
    "        words = []\n",
    "        for i,ind in enumerate(ind_list):\n",
    "            if ind != -1:\n",
    "                words.append(sentence[i])\n",
    "        if len(words)!=0:\n",
    "            cleaned.append(words)\n",
    "    \n",
    "    vec = np.array([w2v_model.wv[q].sum(axis=0)/len(q) for q in cleaned])\n",
    "    y_pred = classifier.predict(vec)\n",
    "    return y_pred.sum()/y_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5833333333333334"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '这看似宁静的大洋里隐藏着不安；那也许是远在洋底的不安的灵魂们；这广阔无垠的牧场起伏不定；多少人永远安身的墓地呀；他们为了自己的梦想而来；为了自己的梦想而死；他们留在这里就像是留在了自己的梦乡；他们翻来覆去；搅得无际的洋面波涛汹涌；这太平洋是世界的心胸；它包裹着我们藉以生存的一切；印度洋和大西洋不过是它的两条手臂；加利福尼亚的护堤更是它不屑摧毁的孩子的沙器'\n",
    "cal_score(s,lr,dictionary,w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '\"那是自然了，我自己的命运当然要由我自己掌握，区区疾病是不能奈我何的。\"'\n",
    "cal_score(s,lr,dictionary,w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1111111111111111"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '为什么会变成这样呢……第一次有了喜欢的人。有了能做一辈子朋友的人。两件快乐事情重合在一起。而这两份快乐，又给我带来更多的快乐。得到的，本该是像梦境一般幸福的时间……但是，为什么，会变成这样呢……'\n",
    "cal_score(s,lr,dictionary,w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二次元分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before undersample:\t [(0.0, 727283), (1.0, 189782)]\n",
      "after undersample:\t [(0.0, 189782), (1.0, 189782)]\n"
     ]
    }
   ],
   "source": [
    "rus = RandomUnderSampler(random_state=0)\n",
    "\n",
    "X = np.concatenate((vec_weibo,vec_wiki,vec_tra,vec_d2),axis = 0)\n",
    "y =  np.concatenate((np.zeros(vec_tra.shape[0] + vec_weibo.shape[0] + vec_wiki.shape[0]),np.ones(vec_d2.shape[0])),axis = 0)\n",
    "\n",
    "print('before undersample:\\t',sorted(Counter(y).items()))\n",
    "\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "\n",
    "print('after undersample:\\t',sorted(Counter(y_resampled).items()))\n",
    "\n",
    "y_resampled = y_resampled.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "C:\\Users\\frank\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8495652937560376"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=test_size, random_state=seed)\n",
    "\n",
    "d2_lr = LogisticRegression()\n",
    "d2_lr = d2_lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = d2_lr.predict(X_test)\n",
    "\n",
    "# check the accuracy on the training set\n",
    "d2_lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '为什么，还要出现在我的面前。为什么，同意我呆在你身边。为什么，总是说些不清不楚的话。为什么，总是摆着一副好像对我一点也不讨厌的态度。为什么，没有来听我的钢琴。为什么，不能接受我啊'\n",
    "cal_score(s,d2_lr,dictionary,w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46153846153846156"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '桥都麻袋,这样子讲话有什么错吗?吶,告诉我啊。搜噶,你们已经不喜欢了啊…真是冷酷的人呢,果咩纳塞,让你看到不愉快的东西了。像我这样的人,果然消失就好了呢。也许只有在二次元的世界里,才有真正的美好存在的吧,吶?'\n",
    "cal_score(s,d2_lr,dictionary,w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 口语化分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before undersample:\t [(0.0, 733457), (1.0, 183608)]\n",
      "after undersample:\t [(0.0, 183608), (1.0, 183608)]\n"
     ]
    }
   ],
   "source": [
    "rus = RandomUnderSampler(random_state=0)\n",
    "\n",
    "X = np.concatenate((vec_wiki,vec_tra,vec_d2,vec_weibo),axis = 0)\n",
    "y =  np.concatenate((np.zeros(vec_tra.shape[0] + vec_d2.shape[0] + vec_wiki.shape[0]),np.ones(vec_weibo.shape[0])),axis = 0)\n",
    "\n",
    "print('before undersample:\\t',sorted(Counter(y).items()))\n",
    "\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "\n",
    "print('after undersample:\\t',sorted(Counter(y_resampled).items()))\n",
    "\n",
    "y_resampled = y_resampled.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "C:\\Users\\frank\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8490782459441171"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=test_size, random_state=seed)\n",
    "\n",
    "oral_lr = LogisticRegression()\n",
    "oral_lr = oral_lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = oral_lr.predict(X_test)\n",
    "\n",
    "# check the accuracy on the training set\n",
    "oral_lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '为什么，还要出现在我的面前。为什么，同意我呆在你身边。为什么，总是说些不清不楚的话。为什么，总是摆着一副好像对我一点也不讨厌的态度。为什么，没有来听我的钢琴。为什么，不能接受我啊'\n",
    "cal_score(s,oral_lr,dictionary,w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学术性分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before undersample:\t [(0.0, 813406), (1.0, 103659)]\n",
      "after undersample:\t [(0.0, 103659), (1.0, 103659)]\n"
     ]
    }
   ],
   "source": [
    "rus = RandomUnderSampler(random_state=0)\n",
    "\n",
    "X = np.concatenate((vec_weibo,vec_tra,vec_d2,vec_wiki),axis = 0)\n",
    "y =  np.concatenate((np.zeros(vec_tra.shape[0] + vec_d2.shape[0] + vec_weibo.shape[0]),np.ones(vec_wiki.shape[0])),axis = 0)\n",
    "\n",
    "print('before undersample:\\t',sorted(Counter(y).items()))\n",
    "\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "\n",
    "print('after undersample:\\t',sorted(Counter(y_resampled).items()))\n",
    "\n",
    "y_resampled = y_resampled.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "C:\\Users\\frank\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9341664839581963"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=test_size, random_state=seed)\n",
    "\n",
    "sch_lr = LogisticRegression()\n",
    "sch_lr = sch_lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = sch_lr.predict(X_test)\n",
    "\n",
    "# check the accuracy on the training set\n",
    "sch_lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。自然语言处理是一门融语言学、计算机科学、数学于一体的科学。因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，所以它与语言学的研究有着密切的联系，但又有重要的区别。自然语言处理并不是一般地研究自然语言，而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。因而它是计算机科学的一部分。'\n",
    "cal_score(s,sch_lr,dictionary,w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文学性分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before undersample:\t [(0.0, 477049), (1.0, 440016)]\n",
      "after undersample:\t [(0.0, 440016), (1.0, 440016)]\n"
     ]
    }
   ],
   "source": [
    "rus = RandomUnderSampler(random_state=0)\n",
    "\n",
    "X = np.concatenate((vec_weibo,vec_d2,vec_wiki,vec_tra),axis = 0)\n",
    "y =  np.concatenate((np.zeros(vec_wiki.shape[0] + vec_d2.shape[0] + vec_weibo.shape[0]),np.ones(vec_tra.shape[0])),axis = 0)\n",
    "\n",
    "print('before undersample:\\t',sorted(Counter(y).items()))\n",
    "\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "\n",
    "print('after undersample:\\t',sorted(Counter(y_resampled).items()))\n",
    "\n",
    "y_resampled = y_resampled.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "C:\\Users\\frank\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8539001621839393"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=test_size, random_state=seed)\n",
    "\n",
    "lite_lr = LogisticRegression()\n",
    "lite_lr = lite_lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lite_lr.predict(X_test)\n",
    "\n",
    "# check the accuracy on the training set\n",
    "lite_lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5769230769230769"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '不到两分钟，甚至更短，他已将全部烦恼给忘记了。就像大人们的烦恼也是烦恼一样，他忘记烦恼并不是因为他的烦恼对他不怎么沉重和难受，而是因为一种新的、更强烈的兴趣暂时压倒并驱散了他心中的烦闷——就像大人们在新奇感受的兴奋之时，也会暂时忘却自己的不幸一样。这种新产生的兴趣就是一种新的吹口哨方法，它很有价值，是刚从一个黑人那学到的，现在他正要一心练习练习又不想被别人打扰。这声音很特别，像小鸟的叫声，一种流畅而委婉的音调。在吹这个调子的时候，舌头断断续续地抵住口腔的上腭——读者若曾经也是孩子的话，也许还记得该怎样吹这种口哨。汤姆学得很勤奋，练得很专心，很快就掌握了其中要领。于是他沿街大步流星地走着，口中吹着口哨，心里乐滋滋的，那股乐劲如同天文学家发现了新行星时一般，仅就乐的程度之深之强烈而言，此时的汤姆绝对比天文学家还要兴奋。'\n",
    "cal_score(s,lite_lr,dictionary,w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 题目文本风格分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questionId</th>\n",
       "      <th>questionTitle</th>\n",
       "      <th>content</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>totalAccepted</th>\n",
       "      <th>totalSubmission</th>\n",
       "      <th>acRate</th>\n",
       "      <th>categoryTitle</th>\n",
       "      <th>likes</th>\n",
       "      <th>translatedTitle</th>\n",
       "      <th>translatedContent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Two Sum</td>\n",
       "      <td>Given an array of integers, return indices of ...</td>\n",
       "      <td>Easy</td>\n",
       "      <td>1179287</td>\n",
       "      <td>2411098</td>\n",
       "      <td>48.9%</td>\n",
       "      <td>Algorithms</td>\n",
       "      <td>8576</td>\n",
       "      <td>两数之和</td>\n",
       "      <td>给定一个整数数组 nums 和一个目标值 target，请你在该数组中找出和为目标值的那 两...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Add Two Numbers</td>\n",
       "      <td>You are given two non-empty linked lists repre...</td>\n",
       "      <td>Medium</td>\n",
       "      <td>467429</td>\n",
       "      <td>1239593</td>\n",
       "      <td>37.7%</td>\n",
       "      <td>Algorithms</td>\n",
       "      <td>4544</td>\n",
       "      <td>两数相加</td>\n",
       "      <td>给出两个 非空 的链表用来表示两个非负的整数。其中，它们各自的位数是按照 逆序 的方式存储的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Longest Substring Without Repeating Characters</td>\n",
       "      <td>Given a string, find the length of the longest...</td>\n",
       "      <td>Medium</td>\n",
       "      <td>551133</td>\n",
       "      <td>1576459</td>\n",
       "      <td>35.0%</td>\n",
       "      <td>Algorithms</td>\n",
       "      <td>3918</td>\n",
       "      <td>无重复字符的最长子串</td>\n",
       "      <td>给定一个字符串，请你找出其中不含有重复字符的 最长子串 的长度。\\n示例 1:\\n输入: \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Median of Two Sorted Arrays</td>\n",
       "      <td>There are two sorted arrays nums1 and nums2 of...</td>\n",
       "      <td>Hard</td>\n",
       "      <td>218703</td>\n",
       "      <td>570697</td>\n",
       "      <td>38.3%</td>\n",
       "      <td>Algorithms</td>\n",
       "      <td>2857</td>\n",
       "      <td>寻找两个正序数组的中位数</td>\n",
       "      <td>给定两个大小为 m 和 n 的正序（从小到大）数组 nums1 和 nums2。\\n请你找出...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Longest Palindromic Substring</td>\n",
       "      <td>Given a string s, find the longest palindromic...</td>\n",
       "      <td>Medium</td>\n",
       "      <td>308517</td>\n",
       "      <td>996465</td>\n",
       "      <td>31.0%</td>\n",
       "      <td>Algorithms</td>\n",
       "      <td>2388</td>\n",
       "      <td>最长回文子串</td>\n",
       "      <td>给定一个字符串 s，找到 s 中最长的回文子串。你可以假设 s 的最大长度为 1000。\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>1622</td>\n",
       "      <td>Max Value of Equation</td>\n",
       "      <td>Given an array points containing the coordinat...</td>\n",
       "      <td>Hard</td>\n",
       "      <td>993</td>\n",
       "      <td>2755</td>\n",
       "      <td>36.0%</td>\n",
       "      <td>Algorithms</td>\n",
       "      <td>7</td>\n",
       "      <td>满足不等式的最大值</td>\n",
       "      <td>给你一个数组 points 和一个整数 k 。数组中每个元素都表示二维平面上的点的坐标，并按...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153</th>\n",
       "      <td>1626</td>\n",
       "      <td>Can Make Arithmetic Progression From Sequence</td>\n",
       "      <td>Given an array of numbers arr. A sequence of n...</td>\n",
       "      <td>Easy</td>\n",
       "      <td>4876</td>\n",
       "      <td>5818</td>\n",
       "      <td>83.8%</td>\n",
       "      <td>Algorithms</td>\n",
       "      <td>0</td>\n",
       "      <td>判断能否形成等差数列</td>\n",
       "      <td>给你一个数字数组 arr 。\\n如果一个数列中，任意相邻两项的差总等于同一个常数，那么这个数...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>1627</td>\n",
       "      <td>Last Moment Before All Ants Fall Out of a Plank</td>\n",
       "      <td>We have a wooden plank of the length n units. ...</td>\n",
       "      <td>Medium</td>\n",
       "      <td>3405</td>\n",
       "      <td>7526</td>\n",
       "      <td>45.2%</td>\n",
       "      <td>Algorithms</td>\n",
       "      <td>8</td>\n",
       "      <td>所有蚂蚁掉下来前的最后一刻</td>\n",
       "      <td>有一块木板，长度为 n 个 单位 。一些蚂蚁在木板上移动，每只蚂蚁都以 每秒一个单位 的速度...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>1628</td>\n",
       "      <td>Count Submatrices With All Ones</td>\n",
       "      <td>Given a rows * columns matrix mat of ones and ...</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2031</td>\n",
       "      <td>4409</td>\n",
       "      <td>46.1%</td>\n",
       "      <td>Algorithms</td>\n",
       "      <td>18</td>\n",
       "      <td>统计全 1 子矩形</td>\n",
       "      <td>给你一个只包含 0 和 1 的 rows * columns 矩阵 mat ，请你返回有多少...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>1629</td>\n",
       "      <td>Minimum Possible Integer After at Most K Adjac...</td>\n",
       "      <td>Given a string num representing the digits of ...</td>\n",
       "      <td>Hard</td>\n",
       "      <td>959</td>\n",
       "      <td>3453</td>\n",
       "      <td>27.8%</td>\n",
       "      <td>Algorithms</td>\n",
       "      <td>9</td>\n",
       "      <td>最多 K 次交换相邻数位后得到的最小整数</td>\n",
       "      <td>给你一个字符串 num 和一个整数 k 。其中，num 表示一个很大的整数，字符串中的每个字...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1157 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      questionId                                      questionTitle  \\\n",
       "0              1                                            Two Sum   \n",
       "1              2                                    Add Two Numbers   \n",
       "2              3     Longest Substring Without Repeating Characters   \n",
       "3              4                        Median of Two Sorted Arrays   \n",
       "4              5                      Longest Palindromic Substring   \n",
       "...          ...                                                ...   \n",
       "1152        1622                              Max Value of Equation   \n",
       "1153        1626      Can Make Arithmetic Progression From Sequence   \n",
       "1154        1627    Last Moment Before All Ants Fall Out of a Plank   \n",
       "1155        1628                    Count Submatrices With All Ones   \n",
       "1156        1629  Minimum Possible Integer After at Most K Adjac...   \n",
       "\n",
       "                                                content difficulty  \\\n",
       "0     Given an array of integers, return indices of ...       Easy   \n",
       "1     You are given two non-empty linked lists repre...     Medium   \n",
       "2     Given a string, find the length of the longest...     Medium   \n",
       "3     There are two sorted arrays nums1 and nums2 of...       Hard   \n",
       "4     Given a string s, find the longest palindromic...     Medium   \n",
       "...                                                 ...        ...   \n",
       "1152  Given an array points containing the coordinat...       Hard   \n",
       "1153  Given an array of numbers arr. A sequence of n...       Easy   \n",
       "1154  We have a wooden plank of the length n units. ...     Medium   \n",
       "1155  Given a rows * columns matrix mat of ones and ...     Medium   \n",
       "1156  Given a string num representing the digits of ...       Hard   \n",
       "\n",
       "      totalAccepted  totalSubmission acRate categoryTitle  likes  \\\n",
       "0           1179287          2411098  48.9%    Algorithms   8576   \n",
       "1            467429          1239593  37.7%    Algorithms   4544   \n",
       "2            551133          1576459  35.0%    Algorithms   3918   \n",
       "3            218703           570697  38.3%    Algorithms   2857   \n",
       "4            308517           996465  31.0%    Algorithms   2388   \n",
       "...             ...              ...    ...           ...    ...   \n",
       "1152            993             2755  36.0%    Algorithms      7   \n",
       "1153           4876             5818  83.8%    Algorithms      0   \n",
       "1154           3405             7526  45.2%    Algorithms      8   \n",
       "1155           2031             4409  46.1%    Algorithms     18   \n",
       "1156            959             3453  27.8%    Algorithms      9   \n",
       "\n",
       "           translatedTitle                                  translatedContent  \n",
       "0                     两数之和  给定一个整数数组 nums 和一个目标值 target，请你在该数组中找出和为目标值的那 两...  \n",
       "1                     两数相加  给出两个 非空 的链表用来表示两个非负的整数。其中，它们各自的位数是按照 逆序 的方式存储的...  \n",
       "2               无重复字符的最长子串  给定一个字符串，请你找出其中不含有重复字符的 最长子串 的长度。\\n示例 1:\\n输入: \"...  \n",
       "3             寻找两个正序数组的中位数  给定两个大小为 m 和 n 的正序（从小到大）数组 nums1 和 nums2。\\n请你找出...  \n",
       "4                   最长回文子串  给定一个字符串 s，找到 s 中最长的回文子串。你可以假设 s 的最大长度为 1000。\\n...  \n",
       "...                    ...                                                ...  \n",
       "1152             满足不等式的最大值  给你一个数组 points 和一个整数 k 。数组中每个元素都表示二维平面上的点的坐标，并按...  \n",
       "1153            判断能否形成等差数列  给你一个数字数组 arr 。\\n如果一个数列中，任意相邻两项的差总等于同一个常数，那么这个数...  \n",
       "1154         所有蚂蚁掉下来前的最后一刻  有一块木板，长度为 n 个 单位 。一些蚂蚁在木板上移动，每只蚂蚁都以 每秒一个单位 的速度...  \n",
       "1155             统计全 1 子矩形  给你一个只包含 0 和 1 的 rows * columns 矩阵 mat ，请你返回有多少...  \n",
       "1156  最多 K 次交换相邻数位后得到的最小整数  给你一个字符串 num 和一个整数 k 。其中，num 表示一个很大的整数，字符串中的每个字...  \n",
       "\n",
       "[1157 rows x 11 columns]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(os.path.join(DATADIR, 'question.csv'), nrows=100000)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df.translatedContent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "lite_score = [0.1 if cal_tra_score(q,lite_lr,dictionary,w2v_model)<0.1 else cal_tra_score(q,lite_lr,dictionary,w2v_model) for q in texts]\n",
    "sch_score = [0.1 if cal_tra_score(q,sch_lr,dictionary,w2v_model)<0.1 else cal_tra_score(q,sch_lr,dictionary,w2v_model) for q in texts]\n",
    "d2_score = [0.1 if cal_tra_score(q,d2_lr,dictionary,w2v_model)<0.1 else cal_tra_score(q,d2_lr,dictionary,w2v_model) for q in texts]\n",
    "oral_score = [0.1 if cal_tra_score(q,oral_lr,dictionary,w2v_model)<0.1 else cal_tra_score(q,oral_lr,dictionary,w2v_model) for q in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {'questionId':df.questionId,'lite_score':lite_score,'sch_score':sch_score,'d2_score':d2_score,'oral_score':oral_score}\n",
    "result_df = pd.DataFrame(data=test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questionId</th>\n",
       "      <th>lite_score</th>\n",
       "      <th>sch_score</th>\n",
       "      <th>d2_score</th>\n",
       "      <th>oral_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>1622</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.677419</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.419355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153</th>\n",
       "      <td>1626</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>1627</td>\n",
       "      <td>0.183673</td>\n",
       "      <td>0.530612</td>\n",
       "      <td>0.448980</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>1628</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>1629</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.294118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1157 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      questionId  lite_score  sch_score  d2_score  oral_score\n",
       "0              1    0.200000   0.900000  0.500000    0.200000\n",
       "1              2    0.142857   0.857143  0.571429    0.142857\n",
       "2              3    0.208333   0.875000  0.541667    0.100000\n",
       "3              4    0.500000   0.750000  0.250000    0.125000\n",
       "4              5    0.111111   0.777778  0.666667    0.333333\n",
       "...          ...         ...        ...       ...         ...\n",
       "1152        1622    0.161290   0.677419  0.129032    0.419355\n",
       "1153        1626    0.142857   0.785714  0.428571    0.100000\n",
       "1154        1627    0.183673   0.530612  0.448980    0.142857\n",
       "1155        1628    0.200000   0.500000  0.400000    0.100000\n",
       "1156        1629    0.117647   0.882353  0.176471    0.294118\n",
       "\n",
       "[1157 rows x 5 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(os.path.join(DATADIR, 'leetcode_result.csv'),index=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
